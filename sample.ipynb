{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f068ae06",
   "metadata": {},
   "source": [
    "# Getting this notebook. \n",
    "\n",
    "Run: \n",
    "\n",
    "*git clone https://github.com/krishnaa423/parallel_tutorial.git*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e80769",
   "metadata": {},
   "source": [
    "# Helper code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7d6f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def write_string_to_file(filename, string):\n",
    "    with open(filename, 'w') as f: f.write(string)\n",
    "    \n",
    "os.environ['MPI_RUN'] = 'mpirun'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2697c",
   "metadata": {},
   "source": [
    "# Tutorial Overview\n",
    "\n",
    "- Python\n",
    "- MPI Hello World\n",
    "- Send and Receive\n",
    "- Race condition\n",
    "- Broadcast\n",
    "- Scatter\n",
    "- Gather\n",
    "- Reduce\n",
    "- Plot computation time vs number of tasks for sum calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf43344",
   "metadata": {},
   "source": [
    "# Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f68ef3",
   "metadata": {},
   "source": [
    "## Variables, Operations, Printing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "677dacd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 2 and 3 is: 5\n"
     ]
    }
   ],
   "source": [
    "# Declare some variables. \n",
    "a = 2\n",
    "b = 3\n",
    "\n",
    "# Add two numbers. \n",
    "c = a + b\n",
    "\n",
    "# Print results. \n",
    "print(f'The sum of {a} and {b} is: {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a308d9b",
   "metadata": {},
   "source": [
    "## Numpy. \n",
    "\n",
    "Stands for numeric python. It is a useful module to do operations on arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7479cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of arrays [1 2 3] and [4 5 6] is: [5 7 9]\n"
     ]
    }
   ],
   "source": [
    "# Import the numpy module. \n",
    "import numpy as np \n",
    "\n",
    "# Create couple arrays. \n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "# Add them. This adds the arrays elementwise. \n",
    "c = a + b\n",
    "\n",
    "# Print the result. \n",
    "print(f'The sum of arrays {a} and {b} is: {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493afc0",
   "metadata": {},
   "source": [
    "# Parallel computing using MPI in Python\n",
    "\n",
    "MPI stands for Message Passing Interface. It can be thought of as a set of functions that can be used to do parallel computations. \n",
    "\n",
    "We will be using Python to demonstate some cool parallel programming concepts. For this, we will use the python module called mpi4py. \n",
    "\n",
    "Some terminology:\n",
    "- Task: Each parallel worker is called a task.\n",
    "- Communicator: A set of tasks that are used to do a computation.\n",
    "\n",
    "It is a little tricky to run MPI code right in the notebook, since we have to usually launch it through a scheduler. To get around that, we will write the code out to a file and then run the file using an MPI launcher. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04494833",
   "metadata": {},
   "source": [
    "## MPI Hello World\n",
    "\n",
    "This example prints out the rank of each task and the size of the communicator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ef949c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[proxy:0@DESKTOP-KFCTBO9] cache_put_flush (proxy/pmip_pmi.c:183): assert (s) failed\n",
      "Hello world from rank 1 of 4\n",
      "Hello world from rank 2 of 4\n",
      "Hello world from rank 3 of 4\n",
      "Hello world from rank 0 of 4\n"
     ]
    }
   ],
   "source": [
    "filename = 'hello_world.py'\n",
    "\n",
    "string = \\\n",
    "'''\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "mpi_rank = comm.Get_rank()\n",
    "mpi_size = comm.Get_size()\n",
    "\n",
    "print(f'Hello world from rank {mpi_rank} of {mpi_size}')\n",
    "'''\n",
    "\n",
    "write_string_to_file(filename, string)\n",
    "\n",
    "!${MPI_RUN} -n 4 python3 hello_world.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a059646",
   "metadata": {},
   "source": [
    "## Send and Receive Arrays\n",
    "\n",
    "We can send an array from one task to an another task. \n",
    "\n",
    "In the example below, we create an array in rank/task=0 and send it to rank/task=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c836f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[proxy:0@DESKTOP-KFCTBO9] cache_put_flush (proxy/pmip_pmi.c:183): assert (s) failed\n",
      "Before send and receive. Rank: 0, a: [0. 1. 2. 3.]\n",
      "Before send and receive. Rank: 1, a: [0. 0. 0. 0.]\n",
      "Before send and receive. Rank: 2, a: [0. 0. 0. 0.]\n",
      "Before send and receive. Rank: 3, a: [0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After send and receive. Rank: 0, a: [0. 1. 2. 3.]\n",
      "After send and receive. Rank: 2, a: [0. 0. 0. 0.]\n",
      "After send and receive. Rank: 3, a: [0. 0. 0. 0.]\n",
      "After send and receive. Rank: 1, a: [0. 1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "filename = 'send_recv.py'\n",
    "\n",
    "string = \\\n",
    "'''\n",
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "mpi_size = comm.Get_size()\n",
    "mpi_rank = comm.Get_rank()\n",
    "\n",
    "# Set array to zero initially. \n",
    "array_size = 4\n",
    "a = np.zeros(shape=(array_size,))\n",
    "\n",
    "# Set to some array before sending and receiving. \n",
    "if mpi_rank==0: \n",
    "    a = np.arange(array_size).astype(dtype='f8')\n",
    "    \n",
    "\n",
    "# Print arrays before the operation.  \n",
    "print(f'Before send and receive. Rank: {mpi_rank}, a: {a}')\n",
    "\n",
    "# Send and Receive. \n",
    "if mpi_rank==0:\n",
    "    comm.Send([a, MPI.DOUBLE], dest=1, tag=10)\n",
    "elif mpi_rank==1:\n",
    "    comm.Recv([a, MPI.DOUBLE], source=0, tag=10)\n",
    "\n",
    "comm.Barrier()\n",
    "print('')\n",
    "\n",
    "# Print arrays after the operation. \n",
    "print(f'After send and receive. Rank: {mpi_rank}, a: {a}') \n",
    "'''\n",
    "\n",
    "write_string_to_file(filename, string)\n",
    "\n",
    "!${MPI_RUN} -n 4 python3 send_recv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f15376",
   "metadata": {},
   "source": [
    "## Race Conditions\n",
    "\n",
    "It is possible to send and receive without waiting for these operations to finish, in case we want to do other operations in the meantime. But it is important to wait for the receive to compelete before using the array. \n",
    "\n",
    "The example below shows the pitfalls of accessing the receive array before waiting for it to be received. We case this a race condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1857a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[proxy:0@DESKTOP-KFCTBO9] cache_put_flush (proxy/pmip_pmi.c:183): assert (s) failed\n",
      "Before send and receive. Rank: 0, a: [0. 1. 2. 3.]\n",
      "Before send and receive. Rank: 1, a: [0. 0. 0. 0.]\n",
      "After send and receive. Rank: 1, a: [0. 0. 0. 0.]\n",
      "After send and receive. Rank: 0, a: [0. 1. 2. 3.]\n",
      "\n",
      "\n",
      "After send and receive and waiting. Rank: 0, a: [0. 1. 2. 3.]\n",
      "After send and receive and waiting. Rank: 1, a: [0. 1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "filename = 'race_condition.py'\n",
    "\n",
    "string = \\\n",
    "'''\n",
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "mpi_size = comm.Get_size()\n",
    "mpi_rank = comm.Get_rank()\n",
    "\n",
    "# Set array to zero initially. \n",
    "array_size = 4\n",
    "a = np.zeros(shape=(array_size,))\n",
    "\n",
    "# Set to some array before sending and receiving. \n",
    "if mpi_rank==0: \n",
    "    a = np.arange(array_size).astype(dtype='f8')\n",
    "    \n",
    "\n",
    "# Print arrays before the operation.  \n",
    "print(f'Before send and receive. Rank: {mpi_rank}, a: {a}')\n",
    "\n",
    "# Send and Receive. \n",
    "if mpi_rank==0:\n",
    "    comm.Isend([a, MPI.DOUBLE], dest=1, tag=10)\n",
    "elif mpi_rank==1:\n",
    "    req = comm.Irecv([a, MPI.DOUBLE], source=0, tag=10)\n",
    "\n",
    "\n",
    "# Print arrays after the operation. \n",
    "print(f'After send and receive. Rank: {mpi_rank}, a: {a}') \n",
    "\n",
    "# Print arrays after waiting. \n",
    "if mpi_rank==1: req.Wait()\n",
    "\n",
    "comm.Barrier()\n",
    "print('')\n",
    "print(f'After send and receive and waiting. Rank: {mpi_rank}, a: {a}') \n",
    "'''\n",
    "\n",
    "write_string_to_file(filename, string)\n",
    "\n",
    "!${MPI_RUN} -n 2 python3 race_condition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb65fac",
   "metadata": {},
   "source": [
    "## Broadcast Operation\n",
    "\n",
    "We can send an array from one rank/task to all ranks/tasks. This is called a broadcast operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a158074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[proxy:0@DESKTOP-KFCTBO9] cache_put_flush (proxy/pmip_pmi.c:183): assert (s) failed\n",
      "Before broadcast. Rank: 3, a: [0. 0. 0. 0.]\n",
      "Before broadcast. Rank: 0, a: [0. 1. 2. 3.]\n",
      "Before broadcast. Rank: 2, a: [0. 0. 0. 0.]\n",
      "Before broadcast. Rank: 1, a: [0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After broadcast. Rank: 0, a: [0. 1. 2. 3.]\n",
      "After broadcast. Rank: 1, a: [0. 1. 2. 3.]\n",
      "After broadcast. Rank: 2, a: [0. 1. 2. 3.]\n",
      "After broadcast. Rank: 3, a: [0. 1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "filename = 'broadcast.py'\n",
    "\n",
    "\n",
    "string = \\\n",
    "'''\n",
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "mpi_size = comm.Get_size()\n",
    "mpi_rank = comm.Get_rank()\n",
    "\n",
    "# Set array to zero initially. \n",
    "array_size = 4\n",
    "a = np.zeros(shape=(array_size,))\n",
    "\n",
    "# Set to some array before broadcast. \n",
    "if mpi_rank==0: \n",
    "    a = np.arange(array_size).astype(dtype='f8')\n",
    "    \n",
    "\n",
    "# Print arrays before the operation.  \n",
    "print(f'Before broadcast. Rank: {mpi_rank}, a: {a}')\n",
    "\n",
    "# Broadcast. \n",
    "comm.Bcast([a, MPI.DOUBLE], root=0)\n",
    "\n",
    "comm.Barrier()\n",
    "print('')\n",
    "\n",
    "# Print arrays after the operation. \n",
    "print(f'After broadcast. Rank: {mpi_rank}, a: {a}') \n",
    "'''\n",
    "\n",
    "write_string_to_file(filename, string)\n",
    "\n",
    "!${MPI_RUN} -n 4 python3 broadcast.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c8120",
   "metadata": {},
   "source": [
    "## Scatter Operation\n",
    "\n",
    "We can split/scatter an array from a rank/task to all the tasks in the communicator. This is done using Scatter operation in MPI, as shown in the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "256639bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[proxy:0@DESKTOP-KFCTBO9] cache_put_flush (proxy/pmip_pmi.c:183): assert (s) failed\n",
      "Before scatter. Rank: 1, a: [0. 0. 0. 0. 0. 0. 0. 0.], b: [0. 0.]\n",
      "Before scatter. Rank: 2, a: [0. 0. 0. 0. 0. 0. 0. 0.], b: [0. 0.]\n",
      "Before scatter. Rank: 0, a: [0. 1. 2. 3. 4. 5. 6. 7.], b: [0. 0.]\n",
      "Before scatter. Rank: 3, a: [0. 0. 0. 0. 0. 0. 0. 0.], b: [0. 0.]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After scatter. Rank: 0, a: [0. 1. 2. 3. 4. 5. 6. 7.], b: [0. 1.]\n",
      "After scatter. Rank: 1, a: [0. 0. 0. 0. 0. 0. 0. 0.], b: [2. 3.]\n",
      "After scatter. Rank: 2, a: [0. 0. 0. 0. 0. 0. 0. 0.], b: [4. 5.]\n",
      "After scatter. Rank: 3, a: [0. 0. 0. 0. 0. 0. 0. 0.], b: [6. 7.]\n"
     ]
    }
   ],
   "source": [
    "filename = 'scatter.py'\n",
    "\n",
    "string = \\\n",
    "'''\n",
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "mpi_size = comm.Get_size()\n",
    "mpi_rank = comm.Get_rank()\n",
    "\n",
    "# Set to some array before scatter operation.\n",
    "array_size = 8\n",
    "if mpi_rank==0: \n",
    "    a = np.arange(array_size, dtype='f8')\n",
    "else:\n",
    "    a = np.zeros(array_size, dtype='f8')\n",
    "b = np.zeros(int(array_size/mpi_size), dtype='f8')\n",
    "\n",
    "# Print arrays before the operation.  \n",
    "print(f'Before scatter. Rank: {mpi_rank}, a: {a}, b: {b}')\n",
    "\n",
    "# Scatter. \n",
    "comm.Scatter(a, b, root=0)\n",
    "\n",
    "comm.Barrier()\n",
    "print('')\n",
    "\n",
    "# Print arrays after the operation. \n",
    "print(f'After scatter. Rank: {mpi_rank}, a: {a}, b: {b}') \n",
    "'''\n",
    "\n",
    "write_string_to_file(filename, string)\n",
    "\n",
    "!${MPI_RUN} -n 4 python3 scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38700728",
   "metadata": {},
   "source": [
    "## Gather Operation\n",
    "\n",
    "\n",
    "Sometimes after scattering the array and letting each rank/task do some operations, we might want to collect the array in a single task. This is done using the Gather operation as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc17f430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[proxy:0@DESKTOP-KFCTBO9] cache_put_flush (proxy/pmip_pmi.c:183): assert (s) failed\n",
      "Before gather. Rank: 0, b: [0. 1.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Before gather. Rank: 1, b: [2. 3.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Before gather. Rank: 2, b: [4. 5.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Before gather. Rank: 3, b: [6. 7.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After gather. Rank: 1, b: [2. 3.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "After gather. Rank: 2, b: [4. 5.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "After gather. Rank: 0, b: [0. 1.], c: [0. 1. 2. 3. 4. 5. 6. 7.]\n",
      "After gather. Rank: 3, b: [6. 7.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "filename='gather.py'\n",
    "\n",
    "string = \\\n",
    "'''\n",
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "mpi_size = comm.Get_size()\n",
    "mpi_rank = comm.Get_rank()\n",
    "\n",
    "# Set to some array before scatter operation.\n",
    "array_size = 8\n",
    "if mpi_rank==0: \n",
    "    a = np.arange(array_size, dtype='f8')\n",
    "else:\n",
    "    a = np.zeros(array_size, dtype='f8')\n",
    "b = np.zeros(int(array_size/mpi_size), dtype='f8')\n",
    "c = np.zeros(array_size, dtype='f8')\n",
    "comm.Scatter(a, b, root=0)\n",
    "\n",
    "# Print arrays before the operation.  \n",
    "print(f'Before gather. Rank: {mpi_rank}, b: {b}, c: {c}')\n",
    "\n",
    "# Gather. \n",
    "comm.Gather(b, c, root=0)\n",
    "\n",
    "comm.Barrier()\n",
    "print('')\n",
    "\n",
    "# Print arrays after the operation. \n",
    "print(f'After gather. Rank: {mpi_rank}, b: {b}, c: {c}') \n",
    "'''\n",
    "\n",
    "write_string_to_file(filename, string)\n",
    "\n",
    "!${MPI_RUN} -n 4 python3 gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed1fdf",
   "metadata": {},
   "source": [
    "## Reduce Operation\n",
    "\n",
    "This example showcases a reduce operation, which sums all the arrays in different ranks/tasks and stores them in the root rank/task, which is 0 in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77b5da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[proxy:0@DESKTOP-KFCTBO9] cache_put_flush (proxy/pmip_pmi.c:183): assert (s) failed\n",
      "Before reduce. Rank: 0, b: [0. 1. 0. 0. 0. 0. 0. 0.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Before reduce. Rank: 1, b: [2. 3. 0. 0. 0. 0. 0. 0.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Before reduce. Rank: 2, b: [4. 5. 0. 0. 0. 0. 0. 0.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Before reduce. Rank: 3, b: [6. 7. 0. 0. 0. 0. 0. 0.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After reduce. Rank: 0, b: [0. 1. 0. 0. 0. 0. 0. 0.], c: [12. 16.  0.  0.  0.  0.  0.  0.]\n",
      "After reduce. Rank: 1, b: [2. 3. 0. 0. 0. 0. 0. 0.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "After reduce. Rank: 2, b: [4. 5. 0. 0. 0. 0. 0. 0.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "After reduce. Rank: 3, b: [6. 7. 0. 0. 0. 0. 0. 0.], c: [0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "filename='reduce.py'\n",
    "\n",
    "string = \\\n",
    "'''\n",
    "from mpi4py import MPI\n",
    "import numpy as np \n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "mpi_size = comm.Get_size()\n",
    "mpi_rank = comm.Get_rank()\n",
    "\n",
    "# Set to some array before scatter operation.\n",
    "array_size = 8\n",
    "if mpi_rank==0: \n",
    "    a = np.arange(array_size, dtype='f8')\n",
    "else:\n",
    "    a = np.zeros(array_size, dtype='f8')\n",
    "b = np.zeros(array_size, dtype='f8')\n",
    "c = np.zeros(array_size, dtype='f8')\n",
    "comm.Scatterv(a, b, root=0)\n",
    "\n",
    "# Print arrays before the operation.  \n",
    "print(f'Before reduce. Rank: {mpi_rank}, b: {b}, c: {c}')\n",
    "\n",
    "# Reduce. \n",
    "comm.Reduce(b, c, root=0)\n",
    "\n",
    "\n",
    "comm.Barrier()\n",
    "print('')\n",
    "\n",
    "# Print arrays after the operation. \n",
    "print(f'After reduce. Rank: {mpi_rank}, b: {b}, c: {c}') \n",
    "'''\n",
    "\n",
    "write_string_to_file(filename, string)\n",
    "\n",
    "!${MPI_RUN} -n 4 python3 reduce.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef531e",
   "metadata": {},
   "source": [
    "## Time for summing an array vs number of MPI tasks\n",
    "\n",
    "This example combines operations above to sum an array efficiently. We create an array in rank/task=0, then scatter it, reduce it, and add the resulting array in the root rank/task. We do this for a set of task sizes and see how much time it takes. \n",
    "\n",
    "Feel free to play around with the *array_size* and *task_sizes* variables in the code below to see how the timings change. \n",
    "\n",
    "What do you observe? Do you see any patterns when array_sizes get smaller or larger?\n",
    "\n",
    "You might notice that as the array size is too small, the performance might get worse with number of tasks, as the communication overhead might overweigh the computational cost. We can say that the calculation is IO bound for small arrays and CPU bound for large arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2149dae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[proxy:0@DESKTOP-KFCTBO9] cache_put_flush (proxy/pmip_pmi.c:183): assert (s) failed\n",
      "ntask: 1, time: 0.018957152000439237, sum: 49999995000000.0\n",
      "ntask: 2, time: 0.015155176999996911, sum: 49999995000000.0\n",
      "ntask: 3, time: 0.019389498000236927, sum: 49999995000000.0\n",
      "ntask: 4, time: 0.017429023000204324, sum: 49999995000000.0\n",
      "ntask: 5, time: 0.017761344000064128, sum: 49999995000000.0\n",
      "ntask: 6, time: 0.018304625999917334, sum: 49999995000000.0\n",
      "ntask: 7, time: 0.018282576000274275, sum: 49999995000000.0\n",
      "ntask: 8, time: 0.019458296000266273, sum: 49999995000000.0\n"
     ]
    }
   ],
   "source": [
    "filename='time_vs_tasks.py'\n",
    "\n",
    "string = \\\n",
    "'''\n",
    "# Some variables. \n",
    "array_size = 10000000\n",
    "task_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "\n",
    "from mpi4py import MPI \n",
    "import numpy as np \n",
    "import time \n",
    "\n",
    "def add_array(array_size, comm: MPI.Comm):\n",
    "    \n",
    "    if comm==MPI.COMM_NULL: return 0, 0\n",
    "    \n",
    "    mpi_size = comm.Get_size()\n",
    "    mpi_rank = comm.Get_rank()\n",
    "    \n",
    "    if mpi_rank==0: \n",
    "        a = np.arange(array_size, dtype='f8')\n",
    "    else:\n",
    "        a = np.zeros(array_size, dtype='f8')\n",
    "    b = np.zeros(array_size, dtype='f8')\n",
    "    c = np.zeros(1, dtype='f8')\n",
    "    sum = np.zeros(1, dtype='f8')\n",
    "    \n",
    "    start_time = MPI.Wtime()\n",
    "    comm.Scatterv(a, b, root=0)\n",
    "    c[0] = np.sum(b)\n",
    "    comm.Barrier()\n",
    "    comm.Reduce(c, sum, op=MPI.SUM, root=0)\n",
    "    \n",
    "    sum = sum[0]\n",
    "    \n",
    "    end_time = MPI.Wtime()\n",
    "    elapsed_time = end_time - start_time\n",
    "        \n",
    "    return sum, elapsed_time\n",
    "\n",
    "def get_comm_array(comm: MPI.Comm, task_sizes: list):\n",
    "    \n",
    "    comm_array = []\n",
    "    \n",
    "    for task_size in task_sizes:\n",
    "        group = comm.Get_group()    \n",
    "        group = group.Incl(range(task_size))\n",
    "        comm_temp = comm.Create(group)\n",
    "        comm_array.append(comm_temp)\n",
    "        \n",
    "    return comm_array\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    global array_size\n",
    "    global task_sizes\n",
    "\n",
    "    comm = MPI.COMM_WORLD\n",
    "    \n",
    "    mpi_size = comm.Get_size()\n",
    "    mpi_rank = comm.Get_rank()\n",
    "    \n",
    "    \n",
    "    \n",
    "    comm_array = get_comm_array(comm, task_sizes)\n",
    "    \n",
    "    ntasks = []\n",
    "    sums = []\n",
    "    times = []\n",
    "    \n",
    "    for comm_entry in comm_array:\n",
    "        sum, elapsed_time = add_array(array_size, comm_entry)\n",
    "        if mpi_rank==0: ntasks.append(comm_entry.Get_size())\n",
    "        if mpi_rank==0: times.append(elapsed_time)\n",
    "        if mpi_rank==0: sums.append(sum)\n",
    "        \n",
    "    # Plot the sums vs time. \n",
    "    if mpi_rank==0:\n",
    "        for ntask, sum, time in zip(ntasks, sums, times):\n",
    "            print(f'ntask: {ntask}, time: {time}, sum: {sum}')\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "write_string_to_file(filename, string)\n",
    "\n",
    "!${MPI_RUN} -n 8 python3 time_vs_tasks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff0f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
